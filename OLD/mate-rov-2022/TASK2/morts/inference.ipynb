{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdvpXXT81Ulf"
      },
      "source": [
        "Import a network trained on fish detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1XpfQKrHabv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871ae234-a04c-446d-c600-35804498abcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fish_detection'...\n",
            "remote: Enumerating objects: 254, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 254 (delta 0), reused 1 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (254/254), 231.35 MiB | 16.38 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "Checking out files: 100% (46/46), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/kwea123/fish_detection.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lZfzyZ30Gdr",
        "outputId": "43524ea0-2d81-47a3-8be8-c736b3909c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFeCLN5x1cX6"
      },
      "source": [
        "# Detecting morts\n",
        "\n",
        "Try to do something similar to this: https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/\n",
        "\n",
        "- Save the predictions of the detector as new images in a new folder to create a training dataset\n",
        "- Manually separate images of live fish from images of morts\n",
        "- Train a network to distinguish between the two\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNUyzN2l2ENb"
      },
      "source": [
        "## Step 1\n",
        "Perform detection on the video and save predicted images\n",
        "### NOT RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIJonDQr16X9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Object_Detector():\n",
        "    def __init__(self, model_path):\n",
        "        self.__load_model(model_path)\n",
        "        print('model loaded')\n",
        "\n",
        "    def __load_model(self, model_path):\n",
        "        self.detection_graph = tf.Graph()\n",
        "        with self.detection_graph.as_default():\n",
        "            od_graph_def = tf.compat.v1.GraphDef()\n",
        "            with tf.compat.v2.io.gfile.GFile(model_path, 'rb') as fid:\n",
        "                serialized_graph = fid.read()\n",
        "                od_graph_def.ParseFromString(serialized_graph)\n",
        "                tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth= True\n",
        "\n",
        "        with self.detection_graph.as_default():\n",
        "            self.sess = tf.compat.v1.Session(config=config, graph=self.detection_graph)\n",
        "            self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "            self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "            self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "        # load label_dict\n",
        "        self.label_dict = {1: 'fish'}\n",
        "        # warmup\n",
        "        self.detect_image(np.ones((600, 600, 3)))\n",
        "\n",
        "\n",
        "    def detect_image(self, image_np, output_path = None, score_thr=0.5, saved=0, print_time=False, fish_boxes=None, box_thr=0.7, cur_frame=0):\n",
        "        image_w, image_h = image_np.shape[1], image_np.shape[0]\n",
        "    \n",
        "        # Actual detection.\n",
        "        (boxes, scores, classes, num) = self.sess.run(\n",
        "          [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
        "          feed_dict={self.image_tensor: np.expand_dims(image_np, axis=0)})\n",
        "        # Visualization of the results of a detection.\n",
        "        for i, box in enumerate(boxes[scores>score_thr]):\n",
        "            \n",
        "            top_left = (int(box[1]*image_w), int(box[0]*image_h))\n",
        "            bottom_right = (int(box[3]*image_w), int(box[2]*image_h))\n",
        "            fish = image_np[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
        "            fish = cv2.cvtColor(fish, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # write the frame to disk\n",
        "            p = os.path.sep.join([output_path,\n",
        "              \"{}.png\".format(saved)])\n",
        "            cv2.imwrite(p, fish)\n",
        "            #print(\"[INFO] saved {} to disk\".format(p))\n",
        "\n",
        "            cv2.rectangle(image_np, top_left, bottom_right, (0,255,0), 3)\n",
        "            cv2.putText(image_np, self.label_dict[int(classes[0,i])], top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "\n",
        "        return image_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRjk8QB64uHj",
        "outputId": "563fe181-6a4d-459d-b9cb-e3b88f3a0ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'fish_detection'\n",
            "/content/fish_detection\n",
            "model loaded\n"
          ]
        }
      ],
      "source": [
        "%cd fish_detection\n",
        "# MODEL_PATH = 'fish_ssd_fpn_graph/frozen_inference_graph.pb'\n",
        "MODEL_PATH = 'fish_inception_v2_graph/frozen_inference_graph.pb'\n",
        "object_detector = Object_Detector(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/PoliTOcean/fish_videos_train.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xl6equs9QQ_",
        "outputId": "cc3b82a2-398a-4b60-ca2d-c6f173797fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/PoliTOcean/fish_videos_train.zip\n",
            "   creating: fish_videos_train/\n",
            "  inflating: fish_videos_train/fish_practice_video_1 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_2 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_3 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_4 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_6 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_7 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_8 (1080p).mp4  \n",
            "  inflating: fish_videos_train/fish_practice_video_9 (1080p).mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ1-ELJU5Akw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9983c90-01de-4f3c-cc2a-a4761d04ce5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_2 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▎        | 1/8 [02:47<19:34, 167.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_8 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 2/8 [13:25<44:26, 444.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_6 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 3/8 [15:08<24:00, 288.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_1 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 4/8 [16:16<13:25, 201.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_9 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▎   | 5/8 [38:26<30:24, 608.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_7 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 6/8 [51:31<22:17, 668.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_3 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 7/8 [54:37<08:30, 510.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Working on file fish_practice_video_4 (1080p).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [1:02:11<00:00, 466.44s/it]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_dir = \"/content/fish_detection/fish_videos_train\"\n",
        "\n",
        "directory = os.fsencode(input_dir)\n",
        "    \n",
        "for file in tqdm(os.listdir(directory)):\n",
        "    input_path = os.fsdecode(file)\n",
        "\n",
        "    print(\"### Working on file \" + input_path)\n",
        "\n",
        "    output_path = \"/content/drive/MyDrive/PoliTOcean/out/\" + input_path.split('.')[0]\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    # Create a VideoCapture object and read from input file\n",
        "    # If the input is the camera, pass 0 instead of the video file name\n",
        "    cap = cv2.VideoCapture(input_dir + '/' + input_path)\n",
        "    # Check if camera opened successfully\n",
        "    if (cap.isOpened()== False): \n",
        "      print(\"Error opening video stream or file\")\n",
        "    # Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
        "    # We convert the resolutions from float to integer.\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    # Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
        "    out = cv2.VideoWriter('outpy.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
        "    saved=0\n",
        "    # Read until video is completed\n",
        "    while(cap.isOpened()):\n",
        "      # Capture frame-by-frame\n",
        "      ret, frame = cap.read()\n",
        "      if ret == True:\n",
        "        if saved % 1 == 0:\n",
        "          img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          img_ = object_detector.detect_image(img, score_thr=0.5, output_path=output_path, saved=saved/1)    \n",
        "          # Write the frame into the file 'output.avi'\n",
        "          out.write(img_)\n",
        "        saved += 1\n",
        "\n",
        "      # Break the loop\n",
        "      else: \n",
        "        break\n",
        "\n",
        "# When everything done, release the video capture and video write objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! zip -r /content/drive/MyDrive/PoliTOcean/out.zip /content/drive/MyDrive/PoliTOcean/out"
      ],
      "metadata": {
        "id": "yG72fQjbs2_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "Manually divide saved images into predictions of alive and mort fish (not a funny part)"
      ],
      "metadata": {
        "id": "nrDfML7V8_C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3\n",
        "Create a simple model to be trained on the saved data\n",
        "\n",
        "#### NOT RUN"
      ],
      "metadata": {
        "id": "_kZKeHFX9KKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/PoliTOcean/alive.zip -d ./alive\n",
        "!unzip /content/drive/MyDrive/PoliTOcean/morts.zip -d ./morts\n",
        "!unzip /content/drive/MyDrive/PoliTOcean/fish_videos_test.zip -d ./fish_videos_test"
      ],
      "metadata": {
        "id": "smuDQLHPxi0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./alive/alive | wc -l\n",
        "!ls ./morts/morts | wc -l"
      ],
      "metadata": {
        "id": "18e2WNcBDL2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Model definition\n",
        "###\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K\n",
        "class LivenessNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "    # first CONV => RELU => CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(16, (3, 3), padding=\"same\",\n",
        "\t\t\tinput_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\t\t# second CONV => RELU => CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "  \t# first (and only) set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(64))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "metadata": {
        "id": "AECozdmvnYB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Training the model\n",
        "###\n",
        "\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "dataset_alive = \"alive/alive\"\n",
        "dataset_morts = \"morts/morts\"\n",
        "model_path = \"model\"\n",
        "le_path = \"label_encoder\"\n",
        "plot = \"loss_plot\"\n",
        "\n",
        "# initialize the initial learning rate, batch size, and number of\n",
        "# epochs to train for\n",
        "INIT_LR = 1e-4\n",
        "BS = 8\n",
        "EPOCHS = 100\n",
        "\n",
        "print(\"[INFO] loading images...\")\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for label, dataset in enumerate([dataset_alive, dataset_morts]):\n",
        "  imagePaths = list(paths.list_images(dataset))\n",
        "  for imagePath in imagePaths:\n",
        "    image = cv2.imread(imagePath)\n",
        "    image = cv2.resize(image, (32, 32))\n",
        "    # update the data and labels lists, respectively\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "\n",
        "# convert the data into a NumPy array, then preprocess it by scaling\n",
        "# all pixel intensities to the range [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "# encode the labels (which are currently strings) as integers and then\n",
        "# one-hot encode them\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels, 2)\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.25, random_state=42)\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model = LivenessNet.build(width=32, height=32, depth=3,\n",
        "\tclasses=len(le.classes_))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "# train the network\n",
        "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
        "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS),\n",
        "\tvalidation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
        "\tepochs=EPOCHS)\n",
        "\n",
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(x=testX, batch_size=BS)\n",
        "# save the network to disk\n",
        "model.save(model_path, save_format=\"h5\")\n",
        "# save the label encoder to disk\n",
        "f = open(le_path, \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(plot)"
      ],
      "metadata": {
        "id": "NoYl3UGtoY73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07f2887-335f-4cf5-8ed9-381a7f136bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network for 100 epochs...\n",
            "Epoch 1/100\n",
            "300/300 [==============================] - 17s 15ms/step - loss: 0.7702 - accuracy: 0.6708 - val_loss: 0.6946 - val_accuracy: 0.5169\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.5771 - accuracy: 0.7865 - val_loss: 0.3873 - val_accuracy: 0.8826\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.5098 - accuracy: 0.8191 - val_loss: 0.3134 - val_accuracy: 0.9014\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.4653 - accuracy: 0.8317 - val_loss: 0.3334 - val_accuracy: 0.8851\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.4136 - accuracy: 0.8463 - val_loss: 0.3018 - val_accuracy: 0.8864\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.4067 - accuracy: 0.8571 - val_loss: 0.3029 - val_accuracy: 0.9089\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.4049 - accuracy: 0.8592 - val_loss: 0.2573 - val_accuracy: 0.9126\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3808 - accuracy: 0.8613 - val_loss: 0.2506 - val_accuracy: 0.9201\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3828 - accuracy: 0.8676 - val_loss: 0.2250 - val_accuracy: 0.9189\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3430 - accuracy: 0.8768 - val_loss: 0.2178 - val_accuracy: 0.9189\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3492 - accuracy: 0.8805 - val_loss: 0.2010 - val_accuracy: 0.9251\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3404 - accuracy: 0.8801 - val_loss: 0.2159 - val_accuracy: 0.9226\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3167 - accuracy: 0.8931 - val_loss: 0.2002 - val_accuracy: 0.9276\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2798 - accuracy: 0.9006 - val_loss: 0.1834 - val_accuracy: 0.9301\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3009 - accuracy: 0.9006 - val_loss: 0.1735 - val_accuracy: 0.9338\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3158 - accuracy: 0.8855 - val_loss: 0.1737 - val_accuracy: 0.9176\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.3179 - accuracy: 0.8868 - val_loss: 0.2262 - val_accuracy: 0.9176\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2894 - accuracy: 0.9052 - val_loss: 0.1641 - val_accuracy: 0.9263\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.2726 - accuracy: 0.9031 - val_loss: 0.1573 - val_accuracy: 0.9388\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2827 - accuracy: 0.8993 - val_loss: 0.2590 - val_accuracy: 0.9213\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2820 - accuracy: 0.9039 - val_loss: 0.1625 - val_accuracy: 0.9451\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2843 - accuracy: 0.8922 - val_loss: 0.1658 - val_accuracy: 0.9376\n",
            "Epoch 23/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2752 - accuracy: 0.9085 - val_loss: 0.1554 - val_accuracy: 0.9576\n",
            "Epoch 24/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2792 - accuracy: 0.9006 - val_loss: 0.1485 - val_accuracy: 0.9426\n",
            "Epoch 25/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2580 - accuracy: 0.9106 - val_loss: 0.1476 - val_accuracy: 0.9426\n",
            "Epoch 26/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2625 - accuracy: 0.9052 - val_loss: 0.1321 - val_accuracy: 0.9538\n",
            "Epoch 27/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2781 - accuracy: 0.9031 - val_loss: 0.1622 - val_accuracy: 0.9301\n",
            "Epoch 28/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2426 - accuracy: 0.9185 - val_loss: 0.1259 - val_accuracy: 0.9501\n",
            "Epoch 29/100\n",
            "300/300 [==============================] - 4s 14ms/step - loss: 0.2651 - accuracy: 0.9131 - val_loss: 0.1408 - val_accuracy: 0.9563\n",
            "Epoch 30/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2404 - accuracy: 0.9244 - val_loss: 0.1322 - val_accuracy: 0.9526\n",
            "Epoch 31/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2370 - accuracy: 0.9198 - val_loss: 0.1192 - val_accuracy: 0.9488\n",
            "Epoch 32/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2442 - accuracy: 0.9185 - val_loss: 0.1124 - val_accuracy: 0.9650\n",
            "Epoch 33/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2615 - accuracy: 0.9144 - val_loss: 0.1211 - val_accuracy: 0.9600\n",
            "Epoch 34/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2532 - accuracy: 0.9102 - val_loss: 0.1224 - val_accuracy: 0.9538\n",
            "Epoch 35/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2902 - accuracy: 0.9002 - val_loss: 0.1396 - val_accuracy: 0.9463\n",
            "Epoch 36/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2446 - accuracy: 0.9156 - val_loss: 0.1149 - val_accuracy: 0.9638\n",
            "Epoch 37/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2680 - accuracy: 0.9106 - val_loss: 0.1173 - val_accuracy: 0.9600\n",
            "Epoch 38/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2385 - accuracy: 0.9190 - val_loss: 0.1180 - val_accuracy: 0.9563\n",
            "Epoch 39/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2313 - accuracy: 0.9156 - val_loss: 0.1433 - val_accuracy: 0.9588\n",
            "Epoch 40/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2466 - accuracy: 0.9156 - val_loss: 0.1187 - val_accuracy: 0.9588\n",
            "Epoch 41/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2372 - accuracy: 0.9135 - val_loss: 0.1145 - val_accuracy: 0.9538\n",
            "Epoch 42/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2181 - accuracy: 0.9215 - val_loss: 0.1388 - val_accuracy: 0.9613\n",
            "Epoch 43/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2372 - accuracy: 0.9202 - val_loss: 0.1071 - val_accuracy: 0.9638\n",
            "Epoch 44/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2231 - accuracy: 0.9211 - val_loss: 0.1187 - val_accuracy: 0.9501\n",
            "Epoch 45/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2213 - accuracy: 0.9215 - val_loss: 0.1543 - val_accuracy: 0.9513\n",
            "Epoch 46/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2261 - accuracy: 0.9211 - val_loss: 0.1335 - val_accuracy: 0.9488\n",
            "Epoch 47/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2105 - accuracy: 0.9290 - val_loss: 0.1423 - val_accuracy: 0.9675\n",
            "Epoch 48/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2097 - accuracy: 0.9319 - val_loss: 0.1207 - val_accuracy: 0.9526\n",
            "Epoch 49/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2243 - accuracy: 0.9294 - val_loss: 0.1202 - val_accuracy: 0.9576\n",
            "Epoch 50/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2188 - accuracy: 0.9198 - val_loss: 0.1035 - val_accuracy: 0.9638\n",
            "Epoch 51/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.1153 - val_accuracy: 0.9650\n",
            "Epoch 52/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2258 - accuracy: 0.9185 - val_loss: 0.1143 - val_accuracy: 0.9513\n",
            "Epoch 53/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2218 - accuracy: 0.9248 - val_loss: 0.1038 - val_accuracy: 0.9588\n",
            "Epoch 54/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2330 - accuracy: 0.9227 - val_loss: 0.1154 - val_accuracy: 0.9563\n",
            "Epoch 55/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2032 - accuracy: 0.9290 - val_loss: 0.1094 - val_accuracy: 0.9625\n",
            "Epoch 56/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2134 - accuracy: 0.9298 - val_loss: 0.0934 - val_accuracy: 0.9700\n",
            "Epoch 57/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2251 - accuracy: 0.9202 - val_loss: 0.0960 - val_accuracy: 0.9725\n",
            "Epoch 58/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2116 - accuracy: 0.9231 - val_loss: 0.0883 - val_accuracy: 0.9663\n",
            "Epoch 59/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2072 - accuracy: 0.9307 - val_loss: 0.1226 - val_accuracy: 0.9501\n",
            "Epoch 60/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1824 - accuracy: 0.9361 - val_loss: 0.1574 - val_accuracy: 0.9463\n",
            "Epoch 61/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2344 - accuracy: 0.9185 - val_loss: 0.1008 - val_accuracy: 0.9700\n",
            "Epoch 62/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.0935 - val_accuracy: 0.9650\n",
            "Epoch 63/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1971 - accuracy: 0.9332 - val_loss: 0.1327 - val_accuracy: 0.9663\n",
            "Epoch 64/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2156 - accuracy: 0.9248 - val_loss: 0.1019 - val_accuracy: 0.9663\n",
            "Epoch 65/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2014 - accuracy: 0.9290 - val_loss: 0.1027 - val_accuracy: 0.9763\n",
            "Epoch 66/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1981 - accuracy: 0.9286 - val_loss: 0.1047 - val_accuracy: 0.9650\n",
            "Epoch 67/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2188 - accuracy: 0.9244 - val_loss: 0.1125 - val_accuracy: 0.9663\n",
            "Epoch 68/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1983 - accuracy: 0.9361 - val_loss: 0.1022 - val_accuracy: 0.9576\n",
            "Epoch 69/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2029 - accuracy: 0.9369 - val_loss: 0.0943 - val_accuracy: 0.9700\n",
            "Epoch 70/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2124 - accuracy: 0.9248 - val_loss: 0.0954 - val_accuracy: 0.9688\n",
            "Epoch 71/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1932 - accuracy: 0.9311 - val_loss: 0.1161 - val_accuracy: 0.9563\n",
            "Epoch 72/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1979 - accuracy: 0.9311 - val_loss: 0.1025 - val_accuracy: 0.9688\n",
            "Epoch 73/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1927 - accuracy: 0.9311 - val_loss: 0.1696 - val_accuracy: 0.9463\n",
            "Epoch 74/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1906 - accuracy: 0.9315 - val_loss: 0.0935 - val_accuracy: 0.9638\n",
            "Epoch 75/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2067 - accuracy: 0.9302 - val_loss: 0.1065 - val_accuracy: 0.9700\n",
            "Epoch 76/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2014 - accuracy: 0.9277 - val_loss: 0.1098 - val_accuracy: 0.9750\n",
            "Epoch 77/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1968 - accuracy: 0.9332 - val_loss: 0.1341 - val_accuracy: 0.9625\n",
            "Epoch 78/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1834 - accuracy: 0.9394 - val_loss: 0.0865 - val_accuracy: 0.9713\n",
            "Epoch 79/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.2281 - accuracy: 0.9206 - val_loss: 0.1584 - val_accuracy: 0.9463\n",
            "Epoch 80/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1974 - accuracy: 0.9394 - val_loss: 0.1247 - val_accuracy: 0.9600\n",
            "Epoch 81/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2102 - accuracy: 0.9269 - val_loss: 0.0845 - val_accuracy: 0.9738\n",
            "Epoch 82/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1871 - accuracy: 0.9394 - val_loss: 0.0952 - val_accuracy: 0.9638\n",
            "Epoch 83/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1833 - accuracy: 0.9378 - val_loss: 0.0878 - val_accuracy: 0.9788\n",
            "Epoch 84/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1822 - accuracy: 0.9398 - val_loss: 0.0840 - val_accuracy: 0.9725\n",
            "Epoch 85/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1908 - accuracy: 0.9382 - val_loss: 0.0892 - val_accuracy: 0.9663\n",
            "Epoch 86/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1622 - accuracy: 0.9432 - val_loss: 0.1118 - val_accuracy: 0.9526\n",
            "Epoch 87/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2099 - accuracy: 0.9332 - val_loss: 0.0891 - val_accuracy: 0.9763\n",
            "Epoch 88/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2068 - accuracy: 0.9286 - val_loss: 0.1092 - val_accuracy: 0.9700\n",
            "Epoch 89/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2077 - accuracy: 0.9294 - val_loss: 0.1180 - val_accuracy: 0.9713\n",
            "Epoch 90/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1721 - accuracy: 0.9407 - val_loss: 0.1031 - val_accuracy: 0.9725\n",
            "Epoch 91/100\n",
            "300/300 [==============================] - 4s 12ms/step - loss: 0.1969 - accuracy: 0.9378 - val_loss: 0.0888 - val_accuracy: 0.9688\n",
            "Epoch 92/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1847 - accuracy: 0.9340 - val_loss: 0.1010 - val_accuracy: 0.9675\n",
            "Epoch 93/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1847 - accuracy: 0.9344 - val_loss: 0.0918 - val_accuracy: 0.9763\n",
            "Epoch 94/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1876 - accuracy: 0.9382 - val_loss: 0.0868 - val_accuracy: 0.9688\n",
            "Epoch 95/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1895 - accuracy: 0.9357 - val_loss: 0.0955 - val_accuracy: 0.9638\n",
            "Epoch 96/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1699 - accuracy: 0.9365 - val_loss: 0.0881 - val_accuracy: 0.9775\n",
            "Epoch 97/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1850 - accuracy: 0.9378 - val_loss: 0.1071 - val_accuracy: 0.9700\n",
            "Epoch 98/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1870 - accuracy: 0.9353 - val_loss: 0.1248 - val_accuracy: 0.9650\n",
            "Epoch 99/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.2006 - accuracy: 0.9327 - val_loss: 0.2319 - val_accuracy: 0.9014\n",
            "Epoch 100/100\n",
            "300/300 [==============================] - 4s 13ms/step - loss: 0.1928 - accuracy: 0.9340 - val_loss: 0.0884 - val_accuracy: 0.9750\n",
            "[INFO] evaluating network...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy trained model on top of the detection network\n",
        "\n",
        "To run to perform detection"
      ],
      "metadata": {
        "id": "IQQXIarVzTrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fish_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3bqFGLU0Axv",
        "outputId": "1e78a03d-e283-40bc-c59f-f4fcf89a65af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fish_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/PoliTOcean/fish_videos_test.zip"
      ],
      "metadata": {
        "id": "jJIRErmWHmm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add12fec-fa4e-43d4-9a33-1e6742d230ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/PoliTOcean/fish_videos_test.zip\n",
            "   creating: fish_videos_test/\n",
            "  inflating: fish_videos_test/fish_practice_video_10 (1080p).mp4  \n",
            "  inflating: fish_videos_test/fish_practice_video_11 (1080p).mp4  \n",
            "  inflating: fish_videos_test/fish_practice_video_12 (1080p).mp4  \n",
            "  inflating: fish_videos_test/fish_practice_video_5 (1080p).mp4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Define object detector class and methods\n",
        "###\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils.video import VideoStream\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import argparse\n",
        "import imutils\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "class Object_Detector():\n",
        "    def __init__(self, model_path):\n",
        "        self.__load_model(model_path)\n",
        "        print('model loaded')\n",
        "\n",
        "    def __load_model(self, model_path):\n",
        "        self.detection_graph = tf.Graph()\n",
        "        with self.detection_graph.as_default():\n",
        "            od_graph_def = tf.compat.v1.GraphDef()\n",
        "            with tf.compat.v2.io.gfile.GFile(model_path, 'rb') as fid:\n",
        "                serialized_graph = fid.read()\n",
        "                od_graph_def.ParseFromString(serialized_graph)\n",
        "                tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth= True\n",
        "\n",
        "        with self.detection_graph.as_default():\n",
        "            self.sess = tf.compat.v1.Session(config=config, graph=self.detection_graph)\n",
        "            self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "            self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "            self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "        # load label_dict\n",
        "        self.label_dict = {1: 'fish'}\n",
        "        \n",
        "        # warmup\n",
        "        self.detect_image(np.ones((600, 600, 3)))\n",
        "\n",
        "\n",
        "    def detect_image(self, image_np, score_thr=0.5, aliveness_model=None, le=None):\n",
        "        image_w, image_h = image_np.shape[1], image_np.shape[0]\n",
        "\n",
        "        # Actual detection.\n",
        "        t = time.time()\n",
        "        (boxes, scores, classes, num) = self.sess.run(\n",
        "          [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
        "          feed_dict={self.image_tensor: np.expand_dims(image_np, axis=0)})\n",
        "\n",
        "        \n",
        "        # Visualization of the results of a detection.\n",
        "        for i, box in enumerate(boxes[scores>score_thr]):\n",
        "            top_left = (int(box[1]*image_w), int(box[0]*image_h))\n",
        "            bottom_right = (int(box[3]*image_w), int(box[2]*image_h))\n",
        "            fish = image_np[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
        "            fish = cv2.cvtColor(fish, cv2.COLOR_RGB2BGR)\n",
        "            fish = cv2.resize(fish, (32, 32))\n",
        "            fish = fish.astype(\"float\") / 255.0\n",
        "            fish = img_to_array(fish)\n",
        "            fish = np.expand_dims(fish, axis=0)\n",
        "            # pass the face ROI through the trained liveness detector\n",
        "            # model to determine if the face is \"real\" or \"fake\"\n",
        "            preds = aliveness_model.predict(fish)[0]\n",
        "            #j = np.argmax(preds)\n",
        "            if preds[1] > 0.01:\n",
        "              j = 1\n",
        "            else:\n",
        "              j = 0 \n",
        "            label = le.classes_[j]\n",
        "\n",
        "            # if label == 0:\n",
        "            #   cv2.rectangle(image_np, top_left, bottom_right, (0,255,0), 3)\n",
        "            #   cv2.putText(image_np, \"alive: \" + str(preds), top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "\n",
        "            # else:\n",
        "            if label != 0:\n",
        "              cv2.rectangle(image_np, top_left, bottom_right, (255,0,0), 3)\n",
        "              cv2.putText(image_np, \"mort \" + str(preds), top_left, cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)\n",
        "        img_float32 = np.uint8(image_np)\n",
        "        image_np = cv2.cvtColor(img_float32, cv2.COLOR_RGB2BGR)\n",
        "        return image_np"
      ],
      "metadata": {
        "id": "E4lnRTjmzSwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/label_encoder /content/drive/MyDrive/PoliTOcean/label_encoder"
      ],
      "metadata": {
        "id": "k-IV1v1jwqDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9768ba-64a6-489e-a7c5-ae071a9a6319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/label_encoder': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Load pretrained models\n",
        "###\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/PoliTOcean/model\"\n",
        "le_path = \"/content/drive/MyDrive/PoliTOcean/label_encoder\"\n",
        "\n",
        "# model_path = \"/content/model\"\n",
        "# le_path = \"/content/label_encoder\"\n",
        "\n",
        "aliveness_model = load_model(model_path)\n",
        "le = pickle.loads(open(le_path, \"rb\").read())\n",
        "MODEL_PATH = 'fish_inception_v2_graph/frozen_inference_graph.pb'\n",
        "object_detector = Object_Detector(MODEL_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUh-iqYy0OFC",
        "outputId": "e1eec32a-2ad1-4227-d029-38aff5571ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO\n"
      ],
      "metadata": {
        "id": "tjKijc9a7kkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_input_link = '/content/fish_detection/fish_videos_test/Task 2.2 EXPLORER Station 1 Fish Video.mp4'\n",
        "video_output_link = '/content/drive/MyDrive/PoliTOcean/out/outpy.avi'"
      ],
      "metadata": {
        "id": "Rh35NWE_7o0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Load video and perform detections\n",
        "###\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Create a VideoCapture object and read from input file\n",
        "# If the input is the camera, pass 0 instead of the video file name\n",
        "cap = cv2.VideoCapture(video_input_link)\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if (cap.isOpened()== False): \n",
        "  print(\"Error opening video stream or file\")\n",
        "\n",
        "# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
        "# We convert the resolutions from float to integer.\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
        "out = cv2.VideoWriter(video_output_link,cv2.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\n",
        "\n",
        "# Read until video is completed\n",
        "while(cap.isOpened()):\n",
        "  # Capture frame-by-frame\n",
        "  ret, frame = cap.read()\n",
        "  if ret == True:\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    img_ = object_detector.detect_image(img, score_thr=0.7, aliveness_model=aliveness_model, le=le)\n",
        "    \n",
        "    # Write the frame into the file 'output.avi'\n",
        "    out.write(img_)\n",
        "\n",
        "  # Break the loop\n",
        "  else: \n",
        "    break\n",
        "\n",
        "\n",
        "# When everything done, release the video capture and video write objects\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "vj1DyxXc1Eow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxO7HWJGGnzx",
        "outputId": "5704f3db-b42b-4851-b2ed-5bd3f6701902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1800/1801 [00:51<00:00, 34.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# Visualize saved video\n",
        "from moviepy.editor import *\n",
        "\n",
        "path=\"/content/drive/MyDrive/PoliTOcean/out/outpy.avi\" \n",
        "\n",
        "clip=VideoFileClip(path)\n",
        "clip.ipython_display(width=1000, max_duration=100000000000000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}